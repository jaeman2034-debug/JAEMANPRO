import { useState, useRef, useCallback, useEffect } from 'react'
import { nluService, Entity } from '../services/nluService'

interface AdvancedSpeechRecognitionHook {
  transcript: string
  isListening: boolean
  intent: string | null
  entities: Entity[]
  confidence: number
  startListening: () => void
  stopListening: () => void
  resetTranscript: () => void
  error: string | null
  isProcessing: boolean
}

export const useAdvancedSpeechRecognition = (): AdvancedSpeechRecognitionHook => {
  const [transcript, setTranscript] = useState('')
  const [isListening, setIsListening] = useState(false)
  const [intent, setIntent] = useState<string | null>(null)
  const [entities, setEntities] = useState<Entity[]>([])
  const [confidence, setConfidence] = useState(0)
  const [error, setError] = useState<string | null>(null)
  const [isProcessing, setIsProcessing] = useState(false)
  
  const recognitionRef = useRef<any>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  // const audioChunksRef = useRef<Blob[]>([])

  // 1. recognition ê°ì²´ ìƒì„±
  useEffect(() => {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    if (!SpeechRecognition) {
      setError('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
      return
    }

    const recognition = new SpeechRecognition()
    recognition.continuous = false
    recognition.interimResults = false
    recognition.maxAlternatives = 1
    recognition.lang = 'ko-KR,en-US'

    recognitionRef.current = recognition
  }, [])

  // 2. ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ë“±ë¡
  useEffect(() => {
    const recognition = recognitionRef.current
    if (!recognition) return

    const handleStart = () => {
      console.log('ğŸ¤ ê³ ê¸‰ ìŒì„± ì¸ì‹ ì‹œì‘ë¨')
      setIsListening(true)
      setError(null)
      setIsProcessing(false)
    }

    const handleResult = async (event: any) => {
      console.log('ğŸ¤ ê³ ê¸‰ ìŒì„± ì¸ì‹ ê²°ê³¼:', event)
      
      if (window.speechSynthesis && window.speechSynthesis.speaking) {
        console.log('ğŸ”‡ TTS ì¤‘ì´ë¯€ë¡œ ìŒì„± ì¸ì‹ ê²°ê³¼ ë¬´ì‹œ')
        return
      }

      let finalTranscript = ''
      for (let i = 0; i < event.results.length; i++) {
        const result = event.results[i]
        if (result.isFinal) {
          finalTranscript += result[0].transcript
        }
      }

      if (finalTranscript) {
        console.log('ğŸ¤ ìµœì¢… ê²°ê³¼:', finalTranscript)
        setTranscript(finalTranscript)
        
        // NLU ì²˜ë¦¬
        setIsProcessing(true)
        try {
          const intentResult = await nluService.processVoiceInput(undefined, finalTranscript)
          setIntent(intentResult.intent)
          setEntities(intentResult.entities)
          setConfidence(intentResult.confidence)
          console.log('ğŸ§  NLU ê²°ê³¼:', intentResult)
        } catch (error) {
          console.error('ğŸ§  NLU ì²˜ë¦¬ ì‹¤íŒ¨:', error)
          setError('ìŒì„± ì´í•´ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
        } finally {
          setIsProcessing(false)
        }
      }
    }

    const handleError = (event: any) => {
      console.log('ğŸ¤ ê³ ê¸‰ ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error)
      setIsListening(false)
      setIsProcessing(false)
      setError(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${event.error}`)
    }

    const handleEnd = () => {
      console.log('ğŸ¤ ê³ ê¸‰ ìŒì„± ì¸ì‹ ì¢…ë£Œë¨')
      setIsListening(false)
    }

    recognition.addEventListener('start', handleStart)
    recognition.addEventListener('result', handleResult)
    recognition.addEventListener('error', handleError)
    recognition.addEventListener('end', handleEnd)

    return () => {
      recognition.removeEventListener('start', handleStart)
      recognition.removeEventListener('result', handleResult)
      recognition.removeEventListener('error', handleError)
      recognition.removeEventListener('end', handleEnd)
    }
  }, [])

  // 3. Whisper APIë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ìŒì„± ì¸ì‹ (í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
  // const startListeningWithWhisper = useCallback(async () => {
  //   try {
  //     const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
  //     const mediaRecorder = new MediaRecorder(stream, {
  //       mimeType: 'audio/webm;codecs=opus'
  //     })
  //     
  //     mediaRecorderRef.current = mediaRecorder
  //     audioChunksRef.current = []

  //     mediaRecorder.ondataavailable = (event) => {
  //       if (event.data.size > 0) {
  //         audioChunksRef.current.push(event.data)
  //       }
  //     }

  //     mediaRecorder.onstop = async () => {
  //       const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
  //       setIsProcessing(true)
  //       
  //       try {
  //         const intentResult = await nluService.processVoiceInput(audioBlob)
  //         setTranscript(intentResult.rawText)
  //         setIntent(intentResult.intent)
  //         setEntities(intentResult.entities)
  //         setConfidence(intentResult.confidence)
  //         console.log('ğŸ§  Whisper + NLU ê²°ê³¼:', intentResult)
  //       } catch (error) {
  //         console.error('ğŸ§  Whisper ì²˜ë¦¬ ì‹¤íŒ¨:', error)
  //         setError('ê³ ê¸‰ ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
  //       } finally {
  //         setIsProcessing(false)
  //       }
  //     }

  //     mediaRecorder.start()
  //     setIsListening(true)
  //     setError(null)
  //     
  //     // 10ì´ˆ í›„ ìë™ ì¤‘ì§€
  //     setTimeout(() => {
  //       if (mediaRecorder.state === 'recording') {
  //         mediaRecorder.stop()
  //         stream.getTracks().forEach(track => track.stop())
  //       }
  //     }, 10000)

  //   } catch (error) {
  //     console.error('ğŸ¤ Whisper ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error)
  //     setError('ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
  //     setIsListening(false)
  //   }
  // }, [])

  // 4. ê¸°ë³¸ ìŒì„± ì¸ì‹ ì‹œì‘
  const startListening = useCallback(() => {
    const recognition = recognitionRef.current
    if (!recognition) {
      setError('ìŒì„± ì¸ì‹ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
      return
    }

    try {
      console.log('ğŸ¤ ê¸°ë³¸ ìŒì„± ì¸ì‹ ì‹œì‘')
      recognition.start()
    } catch (error: any) {
      console.error('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error)
      setError(`ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨: ${error.message}`)
      setIsListening(false)
    }
  }, [])

  // 5. ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopListening = useCallback(() => {
    const recognition = recognitionRef.current
    const mediaRecorder = mediaRecorderRef.current
    
    if (recognition) {
      try {
        recognition.stop()
        console.log('ğŸ¤ ê¸°ë³¸ ìŒì„± ì¸ì‹ ì¤‘ì§€')
      } catch (error) {
        console.error('ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ì§€ ì‹¤íŒ¨:', error)
      }
    }
    
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop()
      console.log('ğŸ¤ Whisper ìŒì„± ì¸ì‹ ì¤‘ì§€')
    }
  }, [])

  // 6. íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë¦¬ì…‹
  const resetTranscript = useCallback(() => {
    setTranscript('')
    setIntent(null)
    setEntities([])
    setConfidence(0)
    setError(null)
  }, [])

  return {
    transcript,
    isListening,
    intent,
    entities,
    confidence,
    startListening,
    stopListening,
    resetTranscript,
    error,
    isProcessing
  }
} 